{"meta":{"title":"Embrace Technology","subtitle":null,"description":"learn more,gain more!","author":"tomnic.wang","url":"http://yoursite.com"},"pages":[{"title":"categories","date":"2018-09-30T05:42:44.000Z","updated":"2018-09-30T05:45:21.444Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"关于","date":"2018-09-30T06:13:32.000Z","updated":"2018-09-30T06:17:37.699Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"14年毕业于安徽师范大学，2013.7-2016.3从事java web开发，2016.3-至今从事大数据相关开发"},{"title":"tags","date":"2018-09-30T05:42:13.000Z","updated":"2018-09-30T05:45:01.439Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"mysql数据表设计","slug":"mysql数据表设计","date":"2018-10-30T07:08:20.000Z","updated":"2018-10-30T08:02:25.623Z","comments":true,"path":"2018/10/30/mysql数据表设计/","link":"","permalink":"http://yoursite.com/2018/10/30/mysql数据表设计/","excerpt":"","text":"为什么字段尽可能用NOT NULL，而不是NULLNULL为什么多人用？ 1、NULL是创建数据表时默认的，初级或不知情的或怕麻烦的程序员不会注意这点。 2、很多人员都以为not null 需要更多空间，其实这不是重点。 3、重点是很多程序员觉得NULL在开发不用去判断插入数据，写sql语句的时候更方便快捷。 网上很多资料都有写： Mysql官网文档： “NULL columns require additional space in the rowto record whether their values are NULL. For MyISAM tables, each NULL columntakes one bit extra, rounded up to the nearest byte.” -———————————————- Mysql难以优化引用可空列查询，它会使索引、索引统计和值更加复杂。可空列需要更多的存储空间，还需要mysql内部进行特殊处理。可空列被索引后，每条记录都需要一个额外的字节，还能导致MYisam 中固定大小的索引变成可变大小的索引 ——–这也是《高性能mysql第二版》介绍的 解读： “可空列需要更多的存储空间”：需要一个额外字节作为判断是否为NULL的标志位 “需要mysql内部进行特殊处理”： 这是mysql索引统计，里面有介绍mysql怎么处理NULL。 注意：但把NULL列改为NOT NULL带来的性能提示很小,除非确定它带来了问题,否则不要把它当成优先的优化措施,最重要的是使用的列的类型的适当性 参考：http://blogread.cn/it/article/5967?f=hot3","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"},{"name":"数据库设计","slug":"数据库设计","permalink":"http://yoursite.com/tags/数据库设计/"},{"name":"mysql优化","slug":"mysql优化","permalink":"http://yoursite.com/tags/mysql优化/"}]},{"title":"spring boot 集成 swagger2","slug":"springboot集成swagger2","date":"2018-10-10T06:41:20.000Z","updated":"2018-10-10T06:34:08.866Z","comments":true,"path":"2018/10/10/springboot集成swagger2/","link":"","permalink":"http://yoursite.com/2018/10/10/springboot集成swagger2/","excerpt":"","text":"环境说明spring boot： 1.5.16.RELEASE swagger2： 2.2.0 jdk：1.8 _152 spring boot 集成 swagger2添加Swagger2依赖在pom.xml中加入Swagger2的依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 创建Swagger2配置类在Application.java同级创建Swagger2的配置类Swagger2。 12345678910111213141516171819202122232425262728293031323334353637package com.linezone.biwork;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger2.annotations.EnableSwagger2;@Configuration@EnableSwagger2public class Swagger2 &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(\"com.linezone.biwork.web\")) .paths(PathSelectors.any()) .build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title(\"Spring Boot中使用Swagger2构建RESTful APIs\") .description(\"Spring Boot Swagger2构建\") .termsOfServiceUrl(\"https://wylgeek.github.io/\") .contact(\"tomnic.wang\") .version(\"1.0\") .build(); &#125;&#125; 添加文档内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import com.linezone.biwork.web.model.User;import io.swagger.annotations.ApiImplicitParam;import io.swagger.annotations.ApiImplicitParams;import io.swagger.annotations.ApiOperation;import org.springframework.web.bind.annotation.*;import java.util.*;@RestController@RequestMapping(value=\"/users\") // 通过这里配置使下面的映射都在/users下public class UserController &#123; // 创建线程安全的Map static Map&lt;Long, User&gt; users = Collections.synchronizedMap(new HashMap&lt;Long, User&gt;()); @ApiOperation(value=\"获取用户列表\", notes=\"\") @RequestMapping(value=\"/\", method= RequestMethod.GET) public List&lt;User&gt; getUserList() &#123; // 处理\"/users/\"的GET请求，用来获取用户列表 // 还可以通过@RequestParam从页面中传递参数来进行查询条件或者翻页信息的传递 List&lt;User&gt; r = new ArrayList&lt;User&gt;(users.values()); return r; &#125; @ApiOperation(value=\"创建用户\", notes=\"根据User对象创建用户\") @ApiImplicitParam(name = \"user\", value = \"用户详细实体user\", required = true, dataType = \"User\") @RequestMapping(value=\"/\", method=RequestMethod.POST) public String postUser(@ModelAttribute User user) &#123; // 处理\"/users/\"的POST请求，用来创建User // 除了@ModelAttribute绑定参数之外，还可以通过@RequestParam从页面中传递参数 users.put(user.getId(), user); return \"success\"; &#125; @ApiOperation(value=\"获取用户详细信息\", notes=\"根据url的id来获取用户详细信息\") @ApiImplicitParam(name = \"id\", value = \"用户ID\", required = true, paramType=\"path\",dataType = \"Long\") @RequestMapping(value=\"/&#123;id&#125;\", method=RequestMethod.GET) public User getUser(@PathVariable Long id) &#123; // 处理\"/users/&#123;id&#125;\"的GET请求，用来获取url中id值的User信息 // url中的id可通过@PathVariable绑定到函数的参数中 return users.get(id); &#125; @ApiOperation(value=\"更新用户详细信息\", notes=\"根据url的id来指定更新对象，并根据传过来的user信息来更新用户详细信息\") @ApiImplicitParams(&#123; @ApiImplicitParam(name = \"id\", value = \"用户ID\", required = true, dataType = \"Long\"), @ApiImplicitParam(name = \"user\", value = \"用户详细实体user\", required = true, dataType = \"User\") &#125;) @RequestMapping(value=\"/&#123;id&#125;\", method=RequestMethod.PUT) public String putUser(@PathVariable Long id, @ModelAttribute User user) &#123; // 处理\"/users/&#123;id&#125;\"的PUT请求，用来更新User信息 User u = users.get(id); u.setName(user.getName()); u.setAge(user.getAge()); users.put(id, u); return \"success\"; &#125; @ApiOperation(value=\"删除用户\", notes=\"根据url的id来指定删除对象\") @ApiImplicitParam(name = \"id\", value = \"用户ID\", required = true, dataType = \"Long\") @RequestMapping(value=\"/&#123;id&#125;\", method=RequestMethod.DELETE) public String deleteUser(@PathVariable Long id) &#123; // 处理\"/users/&#123;id&#125;\"的DELETE请求，用来删除User users.remove(id); return \"success\"; &#125; &#125; 完成上述代码添加上，启动Spring Boot程序，访问：http://localhost:port/swagger-ui.html。就能看到前文所展示的RESTful API的页面。我们可以再点开具体的API请求，以POST类型的/users请求为例，可找到上述代码中我们配置的Notes信息以及参数user的描述信息，如下图所示。 swagger2注解说明介绍swagger通过注解表明该接口会生成文档，包括接口名、请求方法、参数、返回信息的等等。 @Api()用于类；表示标识这个类是swagger的资源 @ApiOperation()用于方法；表示一个http请求的操作 @ApiParam()用于方法，参数，字段说明；表示对参数的添加元数据（说明或是否必填等） @ApiModel()用于类表示对类进行说明，用于参数用实体类接收 @ApiModelProperty()用于方法，字段表示对model属性的说明或者数据操作更改 @ApiIgnore()用于类，方法，方法参数表示这个方法或者类被忽略 @ApiImplicitParam() 用于方法表示单独的请求参数 @ApiImplicitParams() 用于方法，包含多个 @ApiImplicitParam 详细介绍@Api()用于类；表示标识这个类是swagger的资源tags–表示说明value–也是说明，可以使用tags替代但是tags如果有多个值，会生成多个list @ApiOperation() 用于方法；表示一个http请求的操作value用于方法描述notes用于提示内容tags可以重新分组（视情况而用）@ApiParam() 用于方法，参数，字段说明；表示对参数的添加元数据（说明或是否必填等）name–参数名value–参数说明required–是否必填 @ApiModel()用于类 ；表示对类进行说明，用于参数用实体类接收value–表示对象名description–描述都可省略@ApiModelProperty()用于方法，字段； 表示对model属性的说明或者数据操作更改value–字段说明name–重写属性名字dataType–重写属性类型required–是否必填example–举例说明hidden–隐藏 @ApiIgnore()用于类或者方法上，可以不被swagger显示在页面上比较简单, 这里不做举例 @ApiImplicitParam() 用于方法表示单独的请求参数@ApiImplicitParams() 用于方法，包含多个 @ApiImplicitParamname–参数mingvalue–参数说明dataType–数据类型paramType–参数类型example–举例说明 swagger2集成问题问题描述：12345678@ApiOperation(value=\"获取用户详细信息\", notes=\"根据url的id来获取用户详细信息\") @ApiImplicitParam(name = \"id\", value = \"用户ID\", required = true, paramType=\"path\",dataType = \"Long\") @RequestMapping(value=\"/&#123;id&#125;\", method=RequestMethod.GET) public User getUser(@PathVariable Long id) &#123; // 处理\"/users/&#123;id&#125;\"的GET请求，用来获取url中id值的User信息 // url中的id可通过@PathVariable绑定到函数的参数中 return users.get(id); &#125; 2018-10-10 13:49:54.478 WARN 2072 — [io-3333-exec-10] .w.s.m.s.DefaultHandlerExceptionResolver : Resolved [org.springframework.web.method.annotation.MethodArgumentTypeMismatchException: Failed to convert value of type ‘java.lang.String’ to required type ‘java.lang.Long’; nested exception is java.lang.NumberFormatException: For input string: “{id}”] 解决办法如果上诉代码没有写paramType = “path”会提示类型转换String convert to Long错误。 参考：http://blog.didispace.com/springbootswagger2/ https://www.cnblogs.com/fengli9998/p/7921601.html","categories":[{"name":"spring boot","slug":"spring-boot","permalink":"http://yoursite.com/categories/spring-boot/"}],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"http://yoursite.com/tags/spring-boot/"},{"name":"swagger2","slug":"swagger2","permalink":"http://yoursite.com/tags/swagger2/"},{"name":"Restful","slug":"Restful","permalink":"http://yoursite.com/tags/Restful/"}]},{"title":"CDH_Hadoop高可用启用与禁用配置","slug":"CDH-Hadoop高可用启用与禁用","date":"2018-09-30T06:41:20.000Z","updated":"2018-10-08T05:08:28.507Z","comments":true,"path":"2018/09/30/CDH-Hadoop高可用启用与禁用/","link":"","permalink":"http://yoursite.com/2018/09/30/CDH-Hadoop高可用启用与禁用/","excerpt":"CDH高可用环境说明linux环境：centos7.4 CDH版本：CDH5.12.0 外置数据库（MariaDB）版本：5.5.56-MariaDB HDFS HA介绍 在Hadoop 2.0.0之前，NameNode是HDFS集群中的单点故障（SPOF）。每个群集都有一个NameNode，如果该机器或进程变得不可用，整个群集将无法使用，直到NameNode重新启动或在单独的计算机上启动。","text":"CDH高可用环境说明linux环境：centos7.4 CDH版本：CDH5.12.0 外置数据库（MariaDB）版本：5.5.56-MariaDB HDFS HA介绍 在Hadoop 2.0.0之前，NameNode是HDFS集群中的单点故障（SPOF）。每个群集都有一个NameNode，如果该机器或进程变得不可用，整个群集将无法使用，直到NameNode重新启动或在单独的计算机上启动。 这在两个主要方面影响了HDFS集群的总体可用性： 对于计划外事件（例如计算机崩溃），在操作员重新启动NameNode之前，群集将不可用。 计划维护事件（如NameNode计算机上的软件或硬件升级）将导致群集停机时间窗口。 HDFS高可用性功能通过提供在具有热备用的主动/被动配置中的同一群集中运行两个冗余NameNode的选项来解决上述问题。这样，在机器崩溃的情况下，可以快速故障转移到新的NameNode，或者为了计划维护，可以进行正常的管理员启动的故障转移。 在典型的HA群集中，两台独立的计算机配置为NameNode。在任何时间点，其中一个NameNode处于活动状态，另一个处于待机状态。 Active NameNode负责集群中的所有客户端操作，而Standby只是充当从属服务器，维持足够的状态以在必要时提供快速故障转移。 为了使备用节点保持其状态与活动节点同步，当前实现要求两个节点都可以访问共享存储设备上的目录（例如，来自NAS的NFS安装）。在将来的版本中可能会放宽此限制。 当Active节点执行任何名称空间修改时，它会将修改记录持久地记录到存储在共享目录中的编辑日志文件中。 Standby节点不断观察此目录以进行编辑，并在查看编辑时将其应用于自己的命名空间。如果发生故障转移，备用数据库将确保在将自身升级为活动状态之前已从共享存储中读取所有编辑内容。这可确保在发生故障转移之前完全同步命名空间状态。 为了提供快速故障转移，备用节点还必须具有关于群集中块的位置的最新信息。为了实现这一点，DataNode配置了两个NameNode的位置，并向两者发送块位置信息和心跳。 对于HA群集的正确操作而言，一次只有一个NameNode处于活动状态至关重要。否则，命名空间状态将在两者之间快速分歧，冒着数据丢失或其他不正确结果的风险。为了确保此属性并防止所谓的“裂脑情景”，管理员必须为共享存储配置至少一种防护方法。在故障转移期间，如果无法验证先前的活动节点是否已放弃其活动状态，则防护进程负责切断先前Active对共享编辑存储的访问。这可以防止它对命名空间进行任何进一步的编辑，从而允许新的Active安全地进行故障转移。 启用进入HDFS服务页面，点击“操作”下拉菜单中才“启用 High Availability”按钮。 启用HA之后，集群中会有两个NameNode，需要给两个NameNode取一个代理的名称，默认为nameservice1。 选择需要安装NameNode和JournalNode的主机： 填写NameNode和JournalNode的数据目录，如果目录不存在则需要手动去创建，具体如下图。 点击“继续” 点击“完成”按钮，完成配置。 禁用进入HDFS界面，选择下拉列表的“禁用 High Availability”按钮。 禁用HA之后，集群内只会有一个NameNode，则需要从高可用环境下的两个NameNode中选择一个NameNode，同时需要在一个节点安装SecondaryNameNode，如下图所示： 选择HDFS检查点目录，点击“继续”如下图 点击“完成”按钮，完成配置。 YARN HA介绍 ​ ResourceManager HA通过Active-standby资源管理器对实现。在启动时，每个ResourceManager都处于待机状态;进程已启动，但未加载状态。当其中一个ResourceManagers转换为活动状态时，ResourceManager从指定的状态存储加载内部状态并启动所有内部服务。转换为活动的激励来自管理员（通过CLI）或启用自动故障转移时的集成故障转移控制器。 ​ 运行两个ResourceManagers时，可能会出现一个双脑情况，资源管理器都认为它们处于活动状态。为避免这种情况，只有一个ResourceManager应该能够执行活动操作，而另一个ResourceManager应该被“隔离”。基于ZooKeeper的状态存储（ZKRMStateStore）只允许单个ResourceManager对存储状态进行更改，隐式屏蔽其他ResourceManager。这是通过ResourceManager声明对根znode的独占create-delete权限来完成的。根znode上的ACL是根据为存储配置的ACL自动创建的;在安全集群的情况下，建议为根主机设置ACL，以便两个ResourceManagers共享读写管理员访问权限，但具有独占的create-delete访问权限。防护是隐式的，不需要显式配置（如HDFS和MRv1中的防护一样）。 启用进入YARN服务页面，点击“操作”下拉菜单中才“启用 High Availability”按钮。 选择需要安装ResourceManager的另外一个节点，点击“继续” 之后页面则进行启用HA的操作，操作完成之后，点击“完成”按钮，完成YARN的高可用配置。 YARN高可用启用之后，服务页面如下图所示 禁用进入YARN界面，选择下拉列表的“禁用 High Availability”按钮。 禁用HA之后，集群内只会有一个ResourceManager，则需要从高可用环境下的两个已经装了ResourceManager主机中选择一个，选择好以后，选择“继续” 点击“完成”，完成配置 HBASE HA介绍​ HBase的大多数方面在标准配置中都是高度可用的。集群通常由一个Master和三个或更多RegionServers组成，数据存储在HDFS中。要确保每个组件都具有高可用性，请配置一个或多个备份主服务器。备份主服务器在活动主服务器之外的其他主机上运行。 启用首先进入hbase服务界面，再进入“实例”界面。 点击“添加角色”，进入如下界面 选择Master下方的“选择主机”按钮，选择一台需要安装Master的机器，点击“确认”按钮 选择好Master主机之后，再点击“继续”按钮 添加完成之后，实例页面会多出一个Master的角色的实例，但是该实例是“已停止”状态，需要启动该服务。 点击“启动”，则进行Master服务的启动 启动完成之后，则关闭浮窗页面 完成服务添加之后，就会出现两个Master角色实例，一个是备份状态，一个是活动状态，如下图所示： 禁用禁用Hbase高可用，只需要卸载掉Master角色实例，只剩一个Master角色实例即可，操作如下： 选择备份的Master角色实例，停止该角色实例 点击“停止”按钮 角色实例停止完成之后，点击“关闭”按钮 选中已经停止的Master角色实例，点击“删除”选项 浮窗页面，点击“删除” 完成HA禁用操作","categories":[{"name":"CDH配置","slug":"CDH配置","permalink":"http://yoursite.com/categories/CDH配置/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"config","slug":"config","permalink":"http://yoursite.com/tags/config/"}]},{"title":"HDFS数据平衡相关","slug":"HDFS数据平衡","date":"2018-09-30T02:14:05.000Z","updated":"2018-09-30T05:47:07.924Z","comments":true,"path":"2018/09/30/HDFS数据平衡/","link":"","permalink":"http://yoursite.com/2018/09/30/HDFS数据平衡/","excerpt":"","text":"数据平衡 DataNode间数据平衡 数据不平衡原因： 向现有群集添加了新的DataNode 集群机器磁盘坏死 集群DataNode下线 解决方案： HDFS提供了一个平衡器实用程序，可以分析块放置并平衡DataNode上的数据。平衡器移动块直到认为集群是平衡的，这意味着每个DataNode的利用率（节点上已用空间与节点总容量的比率）不同于集群的利用率（使用的空间比率）集群到集群的总容量）不超过给定的阈值百分比。但平衡器不在单个DataNode上的各个卷之间进行平衡。 DataNode磁盘数据平衡 数据不平衡原因： 当我们往HDFS上写入新的数据块，DataNode 将会使用volume选择策略来为这个块选择存储的地方。目前Hadoop支持两种volume选择策略：round-robin 和 available space（详情参见：HDFS-1804），我们可以通过 dfs.datanode.fsdataset.volume.choosing.policy 参数来设置。默认为循环（round-robin）策略将新块均匀分布在可用磁盘上；而可用空间（ available-space ）策略优先将数据写入具有最大可用空间的磁盘。 默认情况下，DataNode 是使用基于round-robin策略来写入新的数据块。然而在一个长时间运行的集群中，由于HDFS中的大规模文件删除或者通过往DataNode 中添加新的磁盘仍然会导致同一个DataNode中的不同磁盘存储的数据很不均衡。即使你使用的是基于可用空间的策略，卷（volume）不平衡仍可导致较低效率的磁盘I/O。比如所有新增的数据块都会往新增的磁盘上写，在此期间，其他的磁盘会处于空闲状态，这样新的磁盘将会是整个系统的瓶颈。 解决方案： HDFS-1312 在线磁盘均衡器，旨在根据各种指标重新平衡正在运行DataNode上的磁盘数据；离线的脚本平衡脚本 升级HDFS版本到3.0使用HDFS自带的磁盘均衡器(diskbalancer)","categories":[{"name":"Hadoop相关","slug":"Hadoop相关","permalink":"http://yoursite.com/categories/Hadoop相关/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"数据平衡","slug":"数据平衡","permalink":"http://yoursite.com/tags/数据平衡/"}]}]}